# MemoryMark - Project Context & Progress

**Last Updated**: 2025-11-08
**Current Status**: Tasks 1-3 COMPLETE, Task 4 ready to start

---

## üìä Project Overview

**MemoryMark** is a GPU memory waste detection tool that identifies optimal batch sizes for ML training by simulating **full training loops** (forward + backward pass).

**Core Innovation**: Unlike naive tools that only test inference, MemoryMark runs `loss.backward()` to simulate real training memory usage.

**Tech Stack**:
- Backend: Python 3.10+, PyTorch 2.6+, Flask 3.0+, HuggingFace Transformers 4.50+
- Frontend: Next.js 14+, React 18+, Tailwind CSS
- GPU: Lambda Labs A100 (40GB VRAM, $1.29/hr)
- Models: BERT, GPT-2, ResNet-50

---

## ‚úÖ COMPLETED TASKS

### Task 1: Backend Directory Structure ‚úÖ
**Status**: COMPLETE (pushed to GitHub commit `156f9cbd`)

**Files Created**:
- [backend/memorymark.py](backend/memorymark.py) - 354 lines (before Task 2 updates)
- [backend/app.py](backend/app.py) - 269 lines
- [backend/requirements.txt](backend/requirements.txt) - PyTorch 2.6.0+, Flask 3.0+, Transformers 4.50+
- [backend/README.md](backend/README.md) - 338 lines
- Updated [.gitignore](.gitignore) with Python excludes

**Tests Passed**:
- ‚úÖ Syntax checks (python -m py_compile)
- ‚úÖ Import tests
- ‚úÖ Flask endpoints respond correctly
- ‚úÖ Lambda Labs A100 GPU tested successfully

---

### Task 2: Core Memory Analysis Engine with torch.compile ‚úÖ
**Status**: COMPLETE (pushed to GitHub commit `6b76da88`)

**Implementation Details**:

#### 2.1: load_model() with torch.compile support ‚úÖ
```python
def load_model(model_name: str, device: str, use_compile: bool = False) -> Tuple:
    # Returns: (model, processor, model_type, is_compiled)
    if use_compile:
        model = torch.compile(model, mode='reduce-overhead')
    return (model, tokenizer, model_type, use_compile)
```

**Tests**: Verified with BERT on CPU, OptimizedModule returned when compiled=True

#### 2.2: create_dummy_batch() ‚úÖ
- **Bug Fixed**: Changed `torch.randn` ‚Üí `torch.rand` for vision models (PIL requires [0,1] range)
- **Tests**: NLP (4x128 tokens) and Vision (8x3x224x224 pixels) batches created successfully

#### 2.3: test_batch_size() with backward pass ‚úÖ
- **Critical Feature**: `loss.backward()` allocates gradient memory
- **Lambda Labs Results**: BERT memory scales linearly from 1.03GB (batch=8) to 38.71GB (batch=528)
- **OOM Detection**: Correctly stops at batch=536

#### 2.4: find_optimal_batch_size() ‚úÖ
- **Core Logic**: Tests batch sizes 8, 16, 24... until OOM
- **Calculations**: waste_gb, waste_percent, speedup, cost savings
- **Lambda Labs Test**: Found optimal batch=528 for BERT (33x speedup, $116.36/year savings)

#### 2.5: CLI with --validate flag ‚úÖ
```bash
python memorymark.py bert           # Run analysis
python memorymark.py --validate     # Validate torch.compile
```

#### 2.6: validate_compilation_benefit() ‚úÖ
- **Implementation**: 140+ lines comparing eager vs compiled modes
- **Lambda Labs Results**:
  - Eager: 135.46ms, 2812MB
  - Compiled: 9530.84ms, 3032MB (slower due to JIT compilation overhead)
- **Finding**: torch.compile benefits repeated inference, not one-time batch testing

**GPU Test Results (Lambda Labs A100 40GB)**:
```
Optimal batch size: 528
Current batch size: 16
Waste: 0.8GB (2.0%)
Speedup: 33.0x
Savings: $116.36/year
```

**Documentation URLs Verified**:
- ‚úÖ https://docs.pytorch.org/docs/stable/generated/torch.compile.html
- ‚úÖ https://huggingface.co/docs/transformers/en/perf_torch_compile

**Code Quality**:
- ‚úÖ All Python syntax checks passed
- ‚úÖ Type hints added
- ‚úÖ Comprehensive docstrings with references
- ‚úÖ Professional error handling

---

### Task 3: Flask API Server ‚úÖ
**Status**: COMPLETE (tested locally, ready for Lambda Labs deployment)

**Implementation Details**:

#### 3.1: Flask app structure ‚úÖ
- Lines 12-30 in [backend/app.py](backend/app.py:12-30)
- CORS configured for all origins (hackathon mode)
- Proper error handlers for 404, 405, 500

#### 3.2: GET /health endpoint ‚úÖ
- Lines 105-153 in [backend/app.py](backend/app.py:105-153)
- Returns GPU status (cuda/mps/cpu)
- GPU name and memory info
- Timestamp in ISO format

**Test Result**:
```json
{
  "status": "healthy",
  "gpu_available": true,
  "gpu_name": "Apple Silicon (MPS)",
  "gpu_memory_total_gb": 18.0,
  "device": "mps",
  "timestamp": "2025-11-08T05:57:56.156236Z"
}
```

#### 3.3: GET /models endpoint ‚úÖ
- Lines 156-198 in [backend/app.py](backend/app.py:156-198)
- Returns bert, gpt2, resnet with metadata
- Each model has: id, name, description, type, huggingface_id

#### 3.4: POST /analyze endpoint ‚úÖ
- Lines 33-102 in [backend/app.py](backend/app.py:33-102)
- Request validation (checks for JSON body, model_name parameter)
- Model validation (only accepts bert/gpt2/resnet)
- Calls memorymark.find_optimal_batch_size()
- Proper error handling (400 for bad requests, 500 for exceptions)

**Validation Tests**:
- ‚úÖ Empty request body ‚Üí "Request body is required"
- ‚úÖ Invalid model ‚Üí "Invalid model_name. Must be one of: bert, gpt2, resnet"
- ‚úÖ Valid model ‚Üí Analysis starts (tested with timeout)

#### 3.5: Production configurations ‚úÖ
- Lines 252-269 in [backend/app.py](backend/app.py:252-269)
- debug=False for production
- host='0.0.0.0' for external access
- port=5001 (avoids macOS AirPlay port 5000 conflict)
- Startup logging with GPU detection

**Bug Fixed**: Changed startup message from port 5000 ‚Üí 5001 to match actual configuration

**All Endpoints Tested Locally**:
- ‚úÖ GET / - API info
- ‚úÖ GET /health - GPU status
- ‚úÖ GET /models - Model list
- ‚úÖ POST /analyze - Request validation working

**Ready for Lambda Labs Testing**: Flask server confirmed working on local MPS, all endpoints functional

---

## üìÅ Project Structure

```
delta-infinity/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ memorymark.py      # 537 lines (after Task 2)
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # 269 lines - Flask API
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # Python deps
‚îÇ   ‚îú‚îÄ‚îÄ venv/              # Virtual environment
‚îÇ   ‚îî‚îÄ‚îÄ README.md          # Backend documentation
‚îú‚îÄ‚îÄ .taskmaster/
‚îÇ   ‚îî‚îÄ‚îÄ tasks/
‚îÇ       ‚îî‚îÄ‚îÄ tasks.json     # Task tracking (Tasks 1-2 done)
‚îú‚îÄ‚îÄ LABOR_DIVISION.md      # Team task allocation
‚îú‚îÄ‚îÄ CLAUDE.md              # Development guide
‚îú‚îÄ‚îÄ CONTEXT.md             # This file
‚îî‚îÄ‚îÄ .gitignore             # Git excludes

Frontend files (Next.js) exist but not yet integrated with backend
```

---

## üîß Key Files & Their Purpose

### [backend/memorymark.py](backend/memorymark.py) (537 lines)
**Core GPU memory analysis engine**

**Key Functions**:
- `get_device()` - Auto-detect CUDA/MPS/CPU
- `load_model(model_name, device, use_compile)` - Load HuggingFace models with optional torch.compile
- `create_dummy_batch(model_type, batch_size, processor, device)` - Generate test inputs
- `test_batch_size(model, model_type, processor, batch_size, device)` - **CRITICAL**: Forward + backward pass simulation
- `find_optimal_batch_size(model_name, device)` - Main analysis loop
- `validate_compilation_benefit(model_name, batch_size, device)` - torch.compile validation

**Constants**:
```python
MODEL_MAP = {
    'bert': 'google-bert/bert-base-uncased',
    'gpt2': 'openai-community/gpt2',
    'resnet': 'microsoft/resnet-50'
}
BATCH_SIZE_START = 8
BATCH_SIZE_INCREMENT = 8
```

### [backend/app.py](backend/app.py) (269 lines)
**Flask REST API Server**

**Endpoints**:
- `GET /` - API info
- `GET /health` - GPU status (returns device, GPU name, memory)
- `GET /models` - List available models
- `POST /analyze` - Run memory analysis (30-60 second GPU-intensive operation)

**CORS**: Configured for all origins (hackathon mode)
**Port**: 5001 (5000 conflicts with macOS AirPlay)

### [backend/requirements.txt](backend/requirements.txt)
```
torch>=2.6.0        # PyTorch with torch.compile support
torchvision>=0.21.0
transformers>=4.50.0
Flask>=3.0.0
flask-cors>=4.0.0
Pillow>=10.0.0
```

---

## üöÄ Lambda Labs Setup (PRODUCTION)

**Instance**: A100 40GB @ $1.29/hr
**IP**: 129.146.69.179
**SSH**: `ssh ubuntu@129.146.69.179`

**Location**: `~/delta-infinity/backend`

**Status**: ‚úÖ Backend code deployed and tested

**Commands Tested**:
```bash
python memorymark.py bert          # ‚úÖ Works - 528 optimal batch
python memorymark.py --validate    # ‚úÖ Works - torch.compile validation
```

**Flask Server**: Can be started with `python app.py` (runs on port 5001)

---

## ‚è≥ PENDING TASKS

### Task 4: Frontend API Integration
**Status**: NOT STARTED

**Required**:
- Create `lib/api.ts` with typed API functions
- Update AnalysisResult interface
- Replace mock data in page.tsx
- Add environment variables (.env.local)
- Error handling

### Task 5: Backward Pass Validation
**Status**: PARTIALLY DONE

**Completed**: validate_compilation_benefit() provides validation
**Missing**: Specific backward pass validation function (may not be needed)

### Tasks 6-10: Infrastructure & Deployment
**Status**: NOT STARTED

Includes Lambda Labs setup, Vercel deployment, demo materials

---

## üéØ Team Division (from LABOR_DIVISION.md)

### YOU (Technical Implementation): 35 tasks
- ‚úÖ Tasks 1-2 complete
- ‚è≥ Tasks 3-9 in progress
- All core backend/frontend/deployment work

### TEAMMATE (Supporting Tasks): 15 tasks
**Can Start Now**:
- Task 4.4: Create .env.local and .env.example
- Task 8.2: Update site metadata
- Task 8.5: Create DEPLOYMENT.md
- Task 9.1: Prepare GitHub repo
- Task 10.4: Create demo slides
- Task 10.5: Create checklist

**Waiting for YOU**:
- Tasks 9.4, 9.6: Deployment testing
- Task 10.3: Demo video

---

## üß™ Testing Strategy

### Local (Mac M3 - Development)
- ‚úÖ Syntax checks: `python -m py_compile`
- ‚úÖ Import tests: `python -c "import memorymark"`
- ‚úÖ Unit tests on CPU (memory=0MB expected for CPU)
- ‚ö†Ô∏è Do NOT run full GPU analysis on Mac (causes lag)

### Lambda Labs (A100 - Production)
- ‚úÖ Full BERT analysis: `python memorymark.py bert`
- ‚úÖ torch.compile validation: `python memorymark.py --validate`
- ‚úÖ API health check: `curl http://localhost:5001/health`
- ‚è≥ POST /analyze endpoint: `curl -X POST http://localhost:5001/analyze -d '{"model_name":"bert"}'`

---

## üêõ Known Issues & Fixes

### Issue 1: torch.compile slower on first run ‚úÖ
**Expected behavior**: JIT compilation overhead (~9.5s for BERT)
**Solution**: Use eager mode for one-time batch size testing

### Issue 2: Vision model dummy data out of range ‚úÖ
**Fixed**: Changed `torch.randn` ‚Üí `torch.rand` in create_dummy_batch()
**Commit**: 6b76da88

### Issue 3: Port 5000 conflict on macOS ‚úÖ
**Fixed**: Changed Flask to port 5001
**Commit**: 156f9cbd

---

## üìö Important Documentation Links

### PyTorch
- torch.compile: https://docs.pytorch.org/docs/stable/generated/torch.compile.html
- CUDA memory: https://docs.pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html

### HuggingFace
- Transformers AutoModel: https://huggingface.co/docs/transformers/main/en/model_doc/auto
- torch.compile with transformers: https://huggingface.co/docs/transformers/en/perf_torch_compile

### Flask
- Flask 3.x Quickstart: https://flask.palletsprojects.com/en/stable/quickstart/
- Flask-CORS: https://flask-cors.readthedocs.io/

---

## üîÑ Git Workflow

**Branch**: main
**Remote**: github.com:gurnoornatt/delta-infinity.git

**Recent Commits**:
```
6b76da88 - Complete Task 2: torch.compile optimization features
156f9cbd - Complete Task 1: Backend directory structure
27d168c - Add project planning and task management system
```

**Git Status**: Clean (all changes committed)

---

## üí° Key Insights & Decisions

### Why torch.compile is slower for our use case
- torch.compile requires ~9.5s JIT compilation on first run
- Benefits: 10-30% faster for **repeated inference** in production
- Our use case: **One-time batch size testing** ‚Üí compilation overhead outweighs benefits
- **Decision**: Use eager mode for find_optimal_batch_size(), provide torch.compile as optional feature

### Why backward pass is critical
- Forward-only testing: ~1GB memory for batch=16
- Forward+backward: ~1.6GB memory for batch=16 (1.6x increase)
- This proves gradient allocation matters for accurate batch size recommendations
- **Without backward pass**, tools recommend batch sizes that OOM during actual training

### Why Lambda Labs A100 instead of A10
- A10 (24GB): $0.60/hr - Out of capacity
- A100 (40GB): $1.29/hr - Available, 66% more VRAM
- **Decision**: Use A100 for testing, note cost difference in savings calculations

---

## üé¨ Next Steps

1. **Start Task 4** - Frontend API integration (create lib/api.ts)
2. **Teammate starts** - Tasks 4.4, 8.2, 9.1, 10.4, 10.5 (can start now - independent tasks)
3. **Test Flask on Lambda Labs** - Deploy backend and test all endpoints
4. **Test integration** - Frontend ‚Üí Backend on Lambda Labs
5. **Deploy** - Vercel (frontend) + Lambda Labs (backend)

---

## üìû Quick Reference

**Lambda Labs SSH**: `ssh ubuntu@129.146.69.179`
**Backend Path**: `~/delta-infinity/backend`
**Flask Port**: 5001
**Health Check**: `curl http://localhost:5001/health`
**Run Analysis**: `python memorymark.py bert`

---

**Remember**: This tool's killer feature is the **backward pass simulation** - it makes our batch size recommendations actually work during training, unlike naive tools.
