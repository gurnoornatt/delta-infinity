{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Create Backend Directory Structure",
        "description": "Set up the Python backend directory with proper structure for the MemoryMark GPU analysis engine",
        "details": "Create backend/ directory with subdirectories. Initialize Python virtual environment. Create memorymark.py (core analysis engine), app.py (Flask server), requirements.txt, and README.md. Set up proper Python project structure following the PRD specifications.",
        "testStrategy": "Verify directory structure matches PRD specification. Test that Python virtual environment activates correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create backend directory and virtual environment",
            "description": "Initialize the backend directory structure and set up Python virtual environment for the MemoryMark project",
            "dependencies": [],
            "details": "Create backend/ directory in project root. Initialize Python virtual environment using 'python3 -m venv venv' inside backend directory. Activate virtual environment and verify Python 3.10+ compatibility. Create .gitignore for Python-specific files (__pycache__, *.pyc, venv/, etc.)",
            "status": "done",
            "testStrategy": "Verify directory structure exists. Test virtual environment activation with 'source venv/bin/activate'. Check Python version is 3.10+."
          },
          {
            "id": 2,
            "title": "Create requirements.txt with modern PyTorch dependencies (Nov 2025)",
            "description": "Define Python dependencies for GPU memory analysis including PyTorch, HuggingFace Transformers, and Flask",
            "dependencies": [],
            "details": "Create requirements.txt with UPDATED November 2025 versions: torch>=2.1.0 (PyTorch 2.x with torch.compile support), transformers>=4.56.0 (latest stable), flask>=3.1.0 (Flask 3.x), flask-cors>=6.0.0 (latest), pillow>=12.0.0 (latest). Pin exact versions for reproducibility. Include GPU-specific PyTorch installation instructions as comments. Recommend Python 3.14 for new projects but maintain 3.10+ minimum compatibility.",
            "status": "done",
            "testStrategy": "Install requirements in virtual environment. Verify torch.cuda.is_available() and torch.compile availability. Test import statements for all dependencies with updated versions."
          },
          {
            "id": 3,
            "title": "Create memorymark.py core engine skeleton",
            "description": "Set up the main memory analysis module with function signatures and basic structure for GPU memory testing",
            "dependencies": [],
            "details": "Create memorymark.py with core functions: load_model(model_name), create_dummy_batch(model, batch_size), test_batch_size(model, batch_size), find_optimal_batch_size(model_name). Include proper imports for torch, transformers. Add GPU availability checks and memory reset functions. Prepare for forward+backward pass implementation.",
            "status": "done",
            "testStrategy": "Verify all imports work. Test GPU detection. Ensure functions can be called without errors (even if not fully implemented)."
          },
          {
            "id": 4,
            "title": "Create Flask app.py server skeleton",
            "description": "Set up Flask API server with endpoint definitions and CORS configuration",
            "dependencies": [],
            "details": "Create app.py with Flask application. Define three endpoints: POST /analyze (accept model_name, return analysis results), GET /health (return GPU status), GET /models (return available models list). Configure flask-cors for frontend integration. Add error handling middleware and JSON response formatting. Set up development server configuration.",
            "status": "done",
            "testStrategy": "Start Flask server and verify endpoints respond. Test CORS headers with curl. Verify JSON response format matches frontend expectations."
          },
          {
            "id": 5,
            "title": "Create backend README.md documentation",
            "description": "Document the backend setup, API endpoints, and development instructions",
            "dependencies": [],
            "details": "Create comprehensive README.md covering: virtual environment setup, dependency installation, GPU requirements, API endpoint documentation with request/response examples, development server startup instructions, troubleshooting common issues (CUDA not available, OOM errors), and testing instructions matching the PRD specifications.",
            "status": "done",
            "testStrategy": "Follow README instructions on fresh environment to verify completeness. Test all documented commands and API examples."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Core Memory Analysis Engine",
        "description": "Build the core memorymark.py module with full training simulation including forward and backward pass",
        "details": "Implement test_batch_size(), create_dummy_batch(), load_model(), and find_optimal_batch_size() functions. Key requirement: Include loss.backward() for gradient computation to simulate full training loops. Use torch.cuda.max_memory_allocated() for accurate memory measurement. MODERN UPDATE: Add torch.compile() optimization support for PyTorch 2.x performance gains. Support BERT, GPT-2, and ResNet-50 models from HuggingFace.",
        "testStrategy": "Validation test: Forward+backward pass should use 2-3x more memory than forward-only. Test with python memorymark.py bert and verify batch size ~32-40 on 24GB GPU.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement load_model() with HuggingFace and torch.compile support",
            "description": "Create function to load BERT, GPT-2, and ResNet models with optional PyTorch 2.x compilation",
            "dependencies": [],
            "details": "Implement load_model(model_name, use_compile=False) that accepts 'bert', 'gpt2', or 'resnet'. Map to HuggingFace model IDs: bert-base-uncased, gpt2, microsoft/resnet-50. Load tokenizer/processor and model. Move model to GPU with .cuda(). Set model to eval mode. MODERN UPDATE: If use_compile=True, apply torch.compile(model, mode='reduce-overhead') for optimized execution. This JIT-compiles the model for faster inference/training. Return (model, processor, model_type, is_compiled) tuple.",
            "status": "done",
            "testStrategy": "Test loading each model with and without compilation. Verify model is on GPU. Test that compiled models work correctly. Check memory allocation difference between compiled vs non-compiled."
          },
          {
            "id": 2,
            "title": "Implement create_dummy_batch() for NLP and vision models",
            "description": "Create function to generate dummy input batches for testing different batch sizes",
            "dependencies": [
              "2.1"
            ],
            "details": "Implement create_dummy_batch(model_type, batch_size, processor). For NLP: create dummy text list, tokenize with max_length=128, return_tensors='pt', move to GPU. For vision: create random tensors (3, 224, 224), process with ImageProcessor, move to GPU. Return dict of inputs ready for model.",
            "status": "done",
            "testStrategy": "Test creating batches of different sizes. Verify tensors are on GPU. Check batch dimensions match expected shape."
          },
          {
            "id": 3,
            "title": "Implement test_batch_size() with FULL training simulation",
            "description": "Core function that tests a specific batch size with forward AND backward pass",
            "dependencies": [
              "2.2"
            ],
            "details": "CRITICAL IMPLEMENTATION: 1) Reset peak memory stats with torch.cuda.reset_peak_memory_stats(). 2) Create dummy batch. 3) Forward pass: outputs = model(**inputs). 4) Create dummy loss: loss = outputs.logits.mean(). 5) BACKWARD PASS: loss.backward() - THIS IS KEY. 6) Measure peak memory: torch.cuda.max_memory_allocated(). 7) Clean up: del outputs, loss, inputs; torch.cuda.empty_cache(); model.zero_grad(). 8) Handle OOM errors gracefully. MODERN UPDATE: Support both standard eager mode and torch.compile optimized models. Return dict with batch_size, memory_mb, memory_gb, success, error, is_compiled.",
            "status": "done",
            "testStrategy": "Verify backward pass runs by checking memory usage is 2-3x forward-only. Test OOM handling with very large batch sizes."
          },
          {
            "id": 4,
            "title": "Implement find_optimal_batch_size() with eager vs compiled comparison",
            "description": "Main function that tests batch sizes in both eager and compiled modes for comprehensive analysis",
            "dependencies": [
              "2.3"
            ],
            "details": "Implement find_optimal_batch_size(model_name, test_compiled=True). 1) Load model. 2) Get GPU total memory. 3) Loop starting at batch_size=8, increment by 8. 4) Call test_batch_size() for each in eager mode. 5) If test_compiled=True, reload model with torch.compile and repeat tests. 6) Stop at OOM for each mode. 7) Calculate metrics: optimal (last successful), current (batch=16), waste_gb, waste_percent, speedup. 8) Calculate cost savings (Lambda Labs $0.60/hr). MODERN UPDATE: Return comparison showing performance/memory difference between eager and compiled modes. Return complete analysis dict matching frontend AnalysisResult interface with compilation_benefit field.",
            "status": "done",
            "testStrategy": "Run full analysis on all 3 models in both modes. Verify BERT finds batch ~32-40. Compare eager vs compiled memory usage and optimal batch sizes. Document compilation benefits."
          },
          {
            "id": 5,
            "title": "Add command-line interface and model constants",
            "description": "Add CLI support and constants for model mapping and configuration",
            "dependencies": [
              "2.4"
            ],
            "details": "Add MODEL_MAP constant with bert/gpt2/resnet mappings. Add BATCH_SIZE_START=8, BATCH_SIZE_INCREMENT=8, etc. Add if __name__ == '__main__' block to accept model name from command line. Add proper error messages for invalid models. Add result printing for CLI usage.",
            "status": "done",
            "testStrategy": "Test CLI: python memorymark.py bert. Verify error handling for invalid models. Check output formatting."
          },
          {
            "id": 6,
            "title": "Add torch.compile performance validation",
            "description": "Validate that torch.compile provides measurable performance improvements",
            "dependencies": [
              "2.4"
            ],
            "details": "Create validate_compilation_benefit() function to compare eager vs compiled model performance. 1) Load model in both modes. 2) Run same batch size test on both. 3) Measure execution time and memory usage. 4) Calculate speedup and memory efficiency. 5) Print comparison report showing torch.compile benefits (typically 10-30% faster, potentially different memory footprint). This validates PyTorch 2.x optimization is working correctly.",
            "status": "done",
            "testStrategy": "Run validation and verify compiled model shows performance improvement. Document actual speedup percentage for each model type."
          }
        ]
      },
      {
        "id": 3,
        "title": "Create Flask API Server",
        "description": "Build Flask API server with endpoints for GPU analysis and health checks",
        "details": "Implement app.py with three endpoints: POST /analyze (runs memory analysis), GET /health (GPU status), GET /models (available models). Add CORS support with flask-cors. Handle errors gracefully and return proper JSON responses. Set timeout handling for 30-60 second analysis runs.",
        "testStrategy": "Test endpoints with curl commands. Verify CORS headers allow frontend requests. Test full analysis workflow with all three models.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Flask app.py server structure",
            "description": "Set up basic Flask application with CORS configuration and error handling framework",
            "dependencies": [],
            "details": "Create app.py file in backend/ directory. Initialize Flask app, configure CORS with flask-cors to allow all origins for hackathon purposes. Set up basic error handling structure and JSON response utilities. Configure server to run on 0.0.0.0:5000 for external access.",
            "status": "done",
            "testStrategy": "Test server starts without errors and CORS headers are present in responses"
          },
          {
            "id": 2,
            "title": "Implement GET /health endpoint",
            "description": "Create health check endpoint that returns GPU status and system information",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement /health endpoint that checks torch.cuda.is_available(), gets GPU device properties (name, memory), and returns JSON response with status, GPU availability, GPU name, total memory in GB, and timestamp. Handle cases where CUDA is not available gracefully.",
            "status": "done",
            "testStrategy": "Test with curl and verify returns proper GPU information or graceful degradation"
          },
          {
            "id": 3,
            "title": "Implement GET /models endpoint",
            "description": "Create endpoint that lists available models for analysis",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement /models endpoint that returns static list of supported models: bert (BERT Base - 110M parameters), gpt2 (GPT-2 - 117M parameters), resnet (ResNet-50 - 25M parameters). Return as JSON array with id, name, description, and type fields for each model.",
            "status": "done",
            "testStrategy": "Test endpoint returns correct model list in expected JSON format"
          },
          {
            "id": 4,
            "title": "Implement POST /analyze endpoint structure",
            "description": "Create analyze endpoint that validates input and calls memorymark analysis",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement /analyze endpoint that accepts JSON with model_name parameter. Validate model_name is one of ['bert', 'gpt2', 'resnet']. Import and call memorymark.find_optimal_batch_size() function. Handle long-running analysis (30-60 seconds) and return complete results as JSON. Add proper error handling for analysis failures.",
            "status": "done",
            "testStrategy": "Test endpoint validates input correctly and handles timeout scenarios appropriately"
          },
          {
            "id": 5,
            "title": "Add production configurations and startup",
            "description": "Configure Flask for production deployment and add startup logging",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "Add production configurations: disable debug mode, configure proper host/port binding for Lambda Labs deployment. Add startup logging that displays server URL, health check endpoint, and available routes. Ensure server handles graceful shutdown and maintains stability during long-running analysis tasks.",
            "status": "done",
            "testStrategy": "Test server starts correctly, logs show proper information, and server remains stable during analysis operations"
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate Real API Backend with Next.js Frontend",
        "description": "Update the existing React frontend to call the real Flask backend instead of using mock data",
        "details": "Create lib/api.ts with typed API functions. Update page.tsx handleAnalyze() to call real backend. Map backend response to frontend AnalysisResult interface. Handle API errors, loading states, and timeouts. The frontend model IDs (bert-base, gpt-2, resnet-50) need to map to backend IDs (bert, gpt2, resnet).",
        "testStrategy": "Test end-to-end flow: select model, click analyze, verify real backend analysis runs and results display correctly. Test error handling for backend failures.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create lib/api.ts with typed API service functions",
            "description": "Create API service module with functions to call Flask backend endpoints",
            "dependencies": [],
            "details": "Create lib/api.ts. Define API_BASE_URL from env (NEXT_PUBLIC_API_URL) with fallback to localhost:5000. Implement analyzeModel(modelId: string), getHealth(), getModels() functions. Add proper TypeScript types. Handle fetch errors and network failures. Add timeout handling (90 seconds for analysis). Map frontend model IDs to backend IDs: bert-base\u2192bert, gpt-2\u2192gpt2, resnet-50\u2192resnet.",
            "status": "done",
            "testStrategy": "Test each API function independently. Mock backend responses. Verify error handling and timeouts work."
          },
          {
            "id": 2,
            "title": "Update AnalysisResult interface to match backend response",
            "description": "Ensure frontend TypeScript interfaces match backend JSON response structure",
            "dependencies": [],
            "details": "Review backend response structure from Flask /analyze endpoint. Update lib/constants.ts AnalysisResult interface if needed. Map backend fields to frontend: optimal_batch_size, optimal_memory_gb (\u2192optimalMemoryUsage), current_memory_gb (\u2192currentMemoryUsage), waste_percent (\u2192wastePercentage), speedup, cost_savings_per_run (\u2192costPerRun), cost_savings_annual (\u2192annualSavings).",
            "status": "done",
            "testStrategy": "TypeScript compilation should pass. Test mapping of all fields from backend to frontend."
          },
          {
            "id": 3,
            "title": "Replace mock data in page.tsx with real API calls",
            "description": "Update handleAnalyze function to call backend instead of using setTimeout mock",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "In app/page.tsx, import analyzeModel from lib/api. Replace the setTimeout mock (lines 19-29) with: const data = await analyzeModel(selectedModel.id). Map the response to match AnalysisResult interface. Add try/catch for error handling. Show user-friendly error messages. Keep loading state management.",
            "status": "done",
            "testStrategy": "Test with backend running: analyze each model and verify real results display. Test with backend down: verify error handling."
          },
          {
            "id": 4,
            "title": "Add environment variable configuration",
            "description": "Setup environment variables for API URL configuration",
            "dependencies": [],
            "details": "Create .env.local file (add to .gitignore). Add NEXT_PUBLIC_API_URL variable. Create .env.example with placeholder. Document in README how to set API URL for local development vs production. For local: http://localhost:5000. For production: http://<LAMBDA_IP>:5000.",
            "status": "done",
            "testStrategy": "Verify env var is accessible in browser via process.env.NEXT_PUBLIC_API_URL. Test with different URLs."
          },
          {
            "id": 5,
            "title": "Add error handling and user feedback",
            "description": "Implement proper error states and user messaging for API failures",
            "dependencies": [
              "4.3"
            ],
            "details": "Add error state to page.tsx. Create error display component or use toast/alert. Handle specific errors: network failure, timeout, backend error, invalid model. Add retry mechanism. Show helpful messages like 'Backend not responding. Is the server running?'. Reset error state when analyzing again.",
            "status": "done",
            "testStrategy": "Test all error scenarios: backend down, timeout, invalid response. Verify user sees clear error messages."
          }
        ]
      },
      {
        "id": 5,
        "title": "Add Backward Pass Validation System",
        "description": "Implement validation testing to ensure backward pass is working correctly",
        "details": "Add validate_backward_pass() function to verify gradient computation is working. Compare forward-only vs forward+backward memory usage. Expected ratio should be 2-3x. Add this validation to memorymark.py as a debug feature and include in testing workflow.",
        "testStrategy": "Run validation test and verify ratio is 2-3x. If ratio ~1x, backward pass is not working (critical bug). Document expected outputs for each model.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement validate_backward_pass() function",
            "description": "Create validation function that compares forward-only vs forward+backward memory usage",
            "dependencies": [],
            "details": "Add validate_backward_pass() to memorymark.py. 1) Load BERT model and tokenizer. 2) Test 1: Forward only - create inputs, run model, measure peak memory. 3) Reset peak stats. 4) Test 2: Forward + Backward - create inputs, run model, create loss, call backward(), measure peak memory. 5) Calculate ratio = forward_backward_mb / forward_only_mb. 6) Print results with pass/fail indicator (expect 2-3x ratio).",
            "status": "pending",
            "testStrategy": "Run validation and verify ratio is 2-3x. If not, backward pass implementation has a bug."
          },
          {
            "id": 2,
            "title": "Add CLI flag for running validation",
            "description": "Add command-line option to run validation test",
            "dependencies": [
              "5.1"
            ],
            "details": "Add argparse or simple sys.argv handling for --validate flag. When present, run validate_backward_pass() instead of normal analysis. Usage: python memorymark.py --validate. Print clear results showing memory usage comparison.",
            "status": "pending",
            "testStrategy": "Test: python memorymark.py --validate. Verify it runs validation and exits."
          },
          {
            "id": 3,
            "title": "Document validation in backend README",
            "description": "Add documentation for validation testing procedure",
            "dependencies": [
              "5.2"
            ],
            "details": "Update backend/README.md with Validation Testing section. Explain what validation tests, why it matters (proves backward pass works), how to run it, what results to expect (2-3x ratio). Include troubleshooting if validation fails.",
            "status": "pending",
            "testStrategy": "Follow README instructions to run validation. Verify documentation is clear and complete."
          }
        ]
      },
      {
        "id": 6,
        "title": "Setup Lambda Labs GPU Environment",
        "description": "Configure Lambda Labs A10 GPU instance for backend deployment",
        "details": "Launch Lambda Labs A10 (24GB) instance with Ubuntu 22.04 + PyTorch. Install dependencies: Python 3.10+, PyTorch 2.0+, transformers, flask, flask-cors. Set up tmux session for persistent server running. Configure SSH access and verify GPU availability with nvidia-smi.",
        "testStrategy": "Verify GPU available with torch.cuda.is_available(). Test basic model loading and memory measurement. Confirm Flask server starts and accepts connections.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Launch Lambda Labs A10 instance",
            "description": "Create and configure Lambda Labs GPU instance",
            "dependencies": [],
            "details": "Go to lambdalabs.com/service/gpu-cloud. Select 1x A10 (24GB) GPU at $0.60/hr. Choose Ubuntu 22.04 LTS with PyTorch preset. Configure SSH keys or password. Launch instance and note public IP address. Wait for instance to be ready (~2-3 minutes).",
            "status": "pending",
            "testStrategy": "Successfully SSH into instance. Run nvidia-smi and verify A10 GPU with 24GB is shown."
          },
          {
            "id": 2,
            "title": "Install system dependencies and Python environment",
            "description": "Setup Python, pip, and essential system tools",
            "dependencies": [
              "6.1"
            ],
            "details": "SSH into instance. Run: sudo apt update && sudo apt install -y python3-pip git tmux. Verify Python 3.10+: python3 --version. Verify CUDA available: python3 -c 'import torch; print(torch.cuda.is_available())' should print True.",
            "status": "pending",
            "testStrategy": "All installations complete without errors. Python and CUDA verification commands succeed."
          },
          {
            "id": 3,
            "title": "Transfer backend code to Lambda Labs instance",
            "description": "Upload or clone backend code to the GPU instance",
            "dependencies": [
              "6.2"
            ],
            "details": "Option 1: Push code to GitHub, then git clone on Lambda. Option 2: Use scp to upload backend/ directory. Create ~/memorymark directory. Transfer memorymark.py, app.py, requirements.txt, README.md. Verify all files are present.",
            "status": "pending",
            "testStrategy": "All backend files present on Lambda instance. Directory structure matches local development."
          },
          {
            "id": 4,
            "title": "Setup Python virtual environment and install dependencies",
            "description": "Create venv and install PyTorch, transformers, Flask",
            "dependencies": [
              "6.3"
            ],
            "details": "cd ~/memorymark. Create venv: python3 -m venv venv. Activate: source venv/bin/activate. Install dependencies: pip install torch transformers flask flask-cors pillow. Verify installations: test imports for each package.",
            "status": "pending",
            "testStrategy": "All packages install successfully. python -c 'import torch, transformers, flask' runs without errors."
          },
          {
            "id": 5,
            "title": "Test backend functionality on GPU instance",
            "description": "Verify memorymark.py and Flask server work on Lambda Labs",
            "dependencies": [
              "6.4"
            ],
            "details": "Test memorymark.py: python memorymark.py bert. Should complete analysis and show results. Test Flask server: python app.py. In another terminal/tmux window: curl http://localhost:5000/health. Should return GPU info JSON.",
            "status": "pending",
            "testStrategy": "CLI analysis completes successfully. Flask server starts and health endpoint returns valid JSON with GPU info."
          }
        ]
      },
      {
        "id": 7,
        "title": "Deploy Backend to Lambda Labs",
        "description": "Deploy the Flask backend to Lambda Labs GPU instance and test production readiness",
        "details": "Upload backend code to Lambda Labs instance. Install requirements in virtual environment. Start Flask server on 0.0.0.0:5000 in tmux session. Test all API endpoints remotely. Configure firewall/security groups for port 5000 access.",
        "testStrategy": "Test remote API calls from local machine. Verify health endpoint returns GPU info. Run full analysis on all three models remotely.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Flask for production deployment",
            "description": "Update app.py settings for production use",
            "dependencies": [],
            "details": "In app.py, ensure: debug=False for production, host='0.0.0.0' for external access, port=5000. Verify CORS allows all origins (or specific Vercel domain). Add production logging. Test locally on Lambda first.",
            "status": "pending",
            "testStrategy": "Flask starts in production mode. CORS headers present. Logs show proper info level messages."
          },
          {
            "id": 2,
            "title": "Start Flask server in tmux session",
            "description": "Run Flask server in persistent tmux session that survives SSH disconnects",
            "dependencies": [
              "7.1"
            ],
            "details": "SSH into Lambda Labs. Activate venv: source venv/bin/activate. Start tmux: tmux new -s memorymark. Run: python app.py. Detach from tmux: Ctrl+B then D. Server continues running. To reattach later: tmux attach -t memorymark.",
            "status": "pending",
            "testStrategy": "Disconnect from SSH. Reconnect and verify Flask still running via tmux attach or curl health endpoint."
          },
          {
            "id": 3,
            "title": "Test external API access from local machine",
            "description": "Verify API is accessible from internet, not just localhost",
            "dependencies": [
              "7.2"
            ],
            "details": "From local machine, test: curl http://<LAMBDA_IP>:5000/health. Should return GPU info. If fails, check Lambda Labs firewall/security groups allow port 5000. Test POST /analyze: curl -X POST http://<LAMBDA_IP>:5000/analyze -H 'Content-Type: application/json' -d '{\"model_name\":\"bert\"}'.",
            "status": "pending",
            "testStrategy": "Both GET /health and POST /analyze work from local machine. Response times acceptable (<60s for analysis)."
          },
          {
            "id": 4,
            "title": "Run full integration tests on all models",
            "description": "Test complete analysis workflow for BERT, GPT-2, and ResNet remotely",
            "dependencies": [
              "7.3"
            ],
            "details": "Test each model via API: bert, gpt2, resnet. Verify responses match expected format. Check memory measurements are reasonable (BERT batch ~32-40). Verify no errors or timeouts. Document actual results for each model.",
            "status": "pending",
            "testStrategy": "All 3 models analyze successfully. Results show proper batch sizes and memory usage. No OOM errors."
          },
          {
            "id": 5,
            "title": "Document production API URL and access",
            "description": "Create documentation for accessing the deployed backend",
            "dependencies": [
              "7.4"
            ],
            "details": "Document Lambda Labs IP and API URL: http://<IP>:5000. Add to .env.example and deployment docs. Note that Lambda Labs IPs may change if instance is restarted. Document how to find current IP. Add API endpoint documentation with curl examples.",
            "status": "pending",
            "testStrategy": "Documentation is clear and accurate. Can access API using documented URL and examples."
          }
        ]
      },
      {
        "id": 8,
        "title": "Configure Frontend for Production with Backend Integration",
        "description": "Setup frontend for Vercel deployment with proper environment variables",
        "details": "Setup Next.js environment variables (NEXT_PUBLIC_API_URL) pointing to Lambda Labs. Update metadata in app/layout.tsx for MemoryMark branding. Test production build. Optimize for Vercel deployment. Document deployment process.",
        "testStrategy": "Run npm run build locally and verify no errors. Test that API_BASE_URL resolves correctly to Lambda Labs IP:5000.",
        "priority": "medium",
        "dependencies": [
          4,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create production environment configuration files",
            "description": "Setup .env files for different deployment environments",
            "dependencies": [],
            "details": "Create .env.local for local development (API_URL=http://localhost:5000). Create .env.production for deployment (will be set in Vercel). Create .env.example documenting all required vars. Add .env.local to .gitignore. Ensure NEXT_PUBLIC_ prefix for client-side access.",
            "status": "pending",
            "testStrategy": "Environment variables accessible in browser. Different configs work for dev vs prod."
          },
          {
            "id": 2,
            "title": "Update site metadata for MemoryMark branding",
            "description": "Replace v0 placeholder branding with MemoryMark identity",
            "dependencies": [],
            "details": "Update app/layout.tsx metadata: title='MemoryMark - GPU Memory Waste Detector', description='Find and fix GPU memory waste in ML training. Analyze BERT, GPT-2, ResNet models. 2-3x speedup, $47k saved.'. Update favicon if needed. Add OpenGraph tags for sharing.",
            "status": "pending",
            "testStrategy": "Browser tab shows MemoryMark title. Meta tags visible in page source. Sharing preview looks correct."
          },
          {
            "id": 3,
            "title": "Test production build locally",
            "description": "Run production build and verify functionality before deployment",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "Run: npm run build. Verify build succeeds without errors or warnings. Run: npm start (production server). Test full workflow: select model, analyze with real Lambda backend, view results. Check console for errors. Verify env vars loaded correctly.",
            "status": "pending",
            "testStrategy": "Build completes successfully. Production build works identically to dev mode. API integration functional."
          },
          {
            "id": 4,
            "title": "Optimize build for Vercel deployment",
            "description": "Ensure Next.js configuration is optimized for Vercel",
            "dependencies": [],
            "details": "Review next.config.ts for production settings. Ensure output: 'standalone' if needed. Check image optimization settings. Verify no build-time environment variable issues. Test bundle size is reasonable. Check for any dynamic imports that might cause issues.",
            "status": "pending",
            "testStrategy": "Build output shows optimized bundles. No warnings about large bundle sizes. Vercel-specific features compatible."
          },
          {
            "id": 5,
            "title": "Create deployment documentation",
            "description": "Document the deployment process and configuration",
            "dependencies": [
              "8.3"
            ],
            "details": "Create or update README with: deployment steps, environment variable setup, Vercel configuration, how to connect to backend, troubleshooting common issues. Include screenshots if helpful. Document how to update API URL when Lambda IP changes.",
            "status": "pending",
            "testStrategy": "Documentation clear and complete. Another developer could deploy following the docs."
          }
        ]
      },
      {
        "id": 9,
        "title": "Deploy Frontend to Vercel",
        "description": "Deploy the React frontend to Vercel with proper configuration",
        "details": "Push code to GitHub repository. Import project to Vercel. Configure environment variable REACT_APP_API_URL with Lambda Labs backend URL. Deploy and test live site. Verify CORS works between Vercel and Lambda Labs.",
        "testStrategy": "Test live site at Vercel URL. Verify model selection, analysis workflow, and results display work end-to-end with real GPU backend.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Prepare GitHub repository for deployment",
            "description": "Push complete codebase to GitHub with proper configuration",
            "dependencies": [],
            "details": "Initialize git if not already: git init. Create .gitignore including: node_modules/, .next/, .env.local, .DS_Store, backend/venv/. Commit all code: git add ., git commit -m 'Initial commit - MemoryMark'. Create GitHub repo. Push: git remote add origin <URL>, git push -u origin main.",
            "status": "pending",
            "testStrategy": "Code successfully pushed to GitHub. All files present except those in .gitignore."
          },
          {
            "id": 2,
            "title": "Import project to Vercel",
            "description": "Connect GitHub repository to Vercel and configure project",
            "dependencies": [
              "9.1"
            ],
            "details": "Go to vercel.com, sign in with GitHub. Click 'New Project'. Import the memorymark repo. Vercel should auto-detect Next.js. Root directory: ./ (or ./frontend if monorepo). Framework preset: Next.js. Build command: npm run build. Output directory: .next.",
            "status": "pending",
            "testStrategy": "Vercel successfully detects project settings. No configuration errors."
          },
          {
            "id": 3,
            "title": "Configure environment variables in Vercel",
            "description": "Add production environment variables to Vercel project settings",
            "dependencies": [
              "9.2"
            ],
            "details": "In Vercel project settings \u2192 Environment Variables. Add: NEXT_PUBLIC_API_URL = http://<LAMBDA_IP>:5000. Ensure it's set for Production, Preview, and Development environments. Save and note that rebuild will be needed for changes to take effect.",
            "status": "pending",
            "testStrategy": "Environment variable visible in Vercel dashboard. Correct Lambda Labs IP configured."
          },
          {
            "id": 4,
            "title": "Deploy and test live site",
            "description": "Trigger deployment and verify live site functionality",
            "dependencies": [
              "9.3"
            ],
            "details": "Click 'Deploy' in Vercel (or push to main triggers auto-deploy). Wait for build to complete (~2-3 min). Visit deployment URL: https://<project>.vercel.app. Test complete workflow: select BERT, click analyze, wait for real results, verify data displays correctly. Test all 3 models.",
            "status": "pending",
            "testStrategy": "Deployment succeeds. Live site loads. All models can be analyzed. Results are real data from Lambda backend."
          },
          {
            "id": 5,
            "title": "Test CORS and cross-origin functionality",
            "description": "Verify Vercel \u2192 Lambda Labs API calls work without CORS issues",
            "dependencies": [
              "9.4"
            ],
            "details": "Open browser DevTools on live site. Attempt analysis. Check Network tab for /analyze request. Verify no CORS errors. Check response headers include Access-Control-Allow-Origin. If CORS errors, update backend Flask CORS config to allow Vercel domain.",
            "status": "pending",
            "testStrategy": "No CORS errors in browser console. API calls succeed from live Vercel site to Lambda backend."
          },
          {
            "id": 6,
            "title": "Test mobile responsiveness and final QA",
            "description": "Verify site works on mobile devices and different browsers",
            "dependencies": [
              "9.4"
            ],
            "details": "Test on mobile device or Chrome DevTools mobile emulation. Verify: layout responsive, buttons clickable, charts/gauges render correctly, text readable. Test on different browsers: Chrome, Safari, Firefox. Test all user flows. Fix any UI issues found.",
            "status": "pending",
            "testStrategy": "Site works on mobile. All features functional. No layout breaks. Cross-browser compatible."
          }
        ]
      },
      {
        "id": 10,
        "title": "Create Demo Materials and Final Testing",
        "description": "Prepare demo script, validation tests, and final system verification",
        "details": "Create demo script following PRD guidelines (5-minute presentation). Record backup demo video. Run validation tests on all models. Verify technical accuracy claims (backward pass working, memory ratios correct). Test mobile responsiveness. Prepare emergency backup plans.",
        "testStrategy": "Complete demo rehearsal. Validate that BERT analysis finds batch_size ~32-40 (not 56). Verify forward+backward uses 2-3x memory of forward-only. Test site on mobile devices.",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Run final validation tests and verify technical accuracy",
            "description": "Execute all validation tests and confirm technical claims are accurate",
            "dependencies": [],
            "details": "Run: python memorymark.py --validate on Lambda Labs. Verify ratio is 2-3x. Test BERT analysis: python memorymark.py bert. Confirm batch size is ~32-40 (not ~56). Document actual results. Test that these match PRD claims. This proves backward pass is working correctly.",
            "status": "pending",
            "testStrategy": "Validation passes with 2-3x ratio. BERT batch size in expected range. Can confidently claim technical accuracy."
          },
          {
            "id": 2,
            "title": "Create 5-minute demo script following PRD structure",
            "description": "Write detailed demo presentation script with timing",
            "dependencies": [],
            "details": "Create demo_script.md with: 0:00-0:30 Hook ('You're wasting 60-70% GPU'), 0:30-2:00 Live demo (analyze BERT, show results), 2:00-3:00 Technical Flex ('We simulate FULL training, not just inference'), 3:00-4:00 Impact ($47k saved, 3x speedup), 4:00-5:00 Close (URL, Q&A). Include what to say, what to show on screen, transitions.",
            "status": "pending",
            "testStrategy": "Script covers all points. Timing fits in 5 minutes. Clear and compelling narrative."
          },
          {
            "id": 3,
            "title": "Practice demo and record backup video",
            "description": "Rehearse presentation and create backup video recording",
            "dependencies": [
              "10.2"
            ],
            "details": "Practice demo 5 times following script. Time each run, adjust as needed. Record screen + audio walkthrough: show live site, run analysis, explain results, highlight technical accuracy. Keep video under 3 minutes. Export in standard format (MP4). Test playback.",
            "status": "pending",
            "testStrategy": "Demo timing consistent ~5 min. Backup video plays correctly. Comfortable with presentation flow."
          },
          {
            "id": 4,
            "title": "Create demo day presentation slides",
            "description": "Design 3-5 slides for demo presentation",
            "dependencies": [],
            "details": "Create slides (Google Slides/PowerPoint/Keynote): 1) Title slide with MemoryMark logo/name, 2) Problem slide (60-70% GPU waste graphic), 3) Solution slide (full training simulation diagram), 4) Impact slide ($47k saved, 3x speedup stats), 5) Optional: Technical Architecture. Use dark theme matching app aesthetic. Export as PDF.",
            "status": "pending",
            "testStrategy": "Slides clear and professional. Support demo narrative. Readable from distance. PDF exports correctly."
          },
          {
            "id": 5,
            "title": "Create demo day checklist and backup plans",
            "description": "Prepare comprehensive checklist and contingency plans",
            "dependencies": [
              "10.3"
            ],
            "details": "Create checklist: Start Lambda instance 30min early, verify /health endpoint, test one analysis, open live site, have backup video ready, slides ready, phone hotspot configured. Backup plans: Plan A (live demo), Plan B (pre-recorded responses), Plan C (terminal demo), Plan D (video only). Prepare answers to likely questions.",
            "status": "pending",
            "testStrategy": "Checklist comprehensive. All backup plans viable. Confident in handling technical issues."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-08T10:21:53.186Z",
      "updated": "2025-11-08T14:24:13.343609Z",
      "description": "Tasks for MemoryMark project - Updated for November 2025 with modern dependencies and torch.compile"
    }
  }
}